{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# BGDRegression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b82a8a83048c1de"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "class MyBGDRegression:\n",
    "    def __init__(self):\n",
    "        self.intercept_ = 0.0\n",
    "        self.coef_ = []\n",
    "\n",
    "    # batch gradient descent\n",
    "    def fit(self, x, y, learningRate = 0.001, noEpochs = 1000, batches = 20):\n",
    "        self.coef_ = [0.0 for _ in range(len(x[0]) + 1)]\n",
    "        for epoch in range(noEpochs):\n",
    "            for i in range(0, len(x), batches):\n",
    "                ycomputed = [self.eval(xi) for xi in x[i:i + batches]]\n",
    "                crtErrors = [yc - yi for yc, yi in\n",
    "                             zip(ycomputed, y[i:i + batches])]\n",
    "                for j in range(0, len(x[0])):\n",
    "                    self.coef_[j] = self.coef_[j] - learningRate * sum(\n",
    "                        [crtError * xi[j] for crtError, xi in zip(crtErrors, x[i:i + batches])])\n",
    "                self.coef_[len(x[0])] = self.coef_[len(x[0])] - learningRate * sum(crtErrors)\n",
    "\n",
    "        self.intercept_ = self.coef_[-1]\n",
    "        self.coef_ = self.coef_[:-1]\n",
    "\n",
    "    def eval(self, xi):\n",
    "        yi = self.coef_[-1]\n",
    "        for j in range(len(xi)):\n",
    "            yi += self.coef_[j] * xi[j]\n",
    "        return yi\n",
    "\n",
    "    def predict(self, x):\n",
    "        yComputed = [self.eval(xi) for xi in x]\n",
    "        return yComputed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T18:14:05.579184900Z",
     "start_time": "2025-04-09T18:14:05.506177300Z"
    }
   },
   "id": "4f836642cecf6d15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9362fd20bc9206c"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MyLogisticRegression1:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=100000, verbose=True, thresholds=[0.5]):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.verbose = verbose\n",
    "        self.theta = None\n",
    "        self.thresholds = thresholds\n",
    "\n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    #masoara cat de bine se potrivesc probabilitatile prezise cu etichetele reale\n",
    "    def __binary_cross_entropy_loss(self, h, y): # Binary Cross-Entropy Loss\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    #este bazata pe o marja\n",
    "    def __hinge_loss(self, h, y, threshold): # Hinge Loss\n",
    "        y_pred = (h >= threshold).astype(int) * 2 - 1\n",
    "        return np.maximum(0, 1 - y * y_pred).mean()\n",
    "    \n",
    "    #masoara diferenta patratica dintre valorile prezise si cele reale\n",
    "    def __mean_squared_error(self, h, y, threshold): # Mean Squared Error\n",
    "        y_pred = (h >= threshold).astype(int)\n",
    "        return ((y_pred - y) ** 2).mean()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.theta = np.zeros(n + 1)\n",
    "\n",
    "        X = np.concatenate((np.ones((m, 1)), X), axis=1)\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / m\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "\n",
    "            if self.verbose and i % 10000 == 0:\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "                # investigarea diferitelor funcÈ›ii de loss (optional)\n",
    "                for threshold in self.thresholds:\n",
    "                    print(f'Loss at iteration {i}: {self.__binary_cross_entropy_loss(h, y)} (Binary Cross-Entropy Loss)')\n",
    "                    print(f'Loss at iteration {i} with threshold {threshold}: {self.__hinge_loss(h, y, threshold)} (Hinge Loss)')\n",
    "                    print(f'Loss at iteration {i} with threshold {threshold}: {self.__mean_squared_error(h, y, threshold)} (Mean Squared Error)')\n",
    "                print()\n",
    "\n",
    "    def predict(self, X):\n",
    "        m = X.shape[0]\n",
    "        X = np.concatenate((np.ones((m, 1)), X), axis=1)\n",
    "        z = np.dot(X, self.theta)\n",
    "        return np.round(self.__sigmoid(z))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T18:14:06.289573Z",
     "start_time": "2025-04-09T18:14:05.515592400Z"
    }
   },
   "id": "dfe7bd267b47696f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression v2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4d5b66445da8a1f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyLogisticRegression2:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, threshold=0.33):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.threshold = threshold\n",
    "        self.theta = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def cost_function(self, X, y, theta):\n",
    "        m = len(y)\n",
    "        h = self.sigmoid(np.dot(X, theta))\n",
    "        epsilon = 1e-5\n",
    "        cost = (1 / m) * (-np.dot(y.T, np.log(h + epsilon)) - np.dot((1 - y).T, np.log(1 - h + epsilon)))\n",
    "        return cost\n",
    "\n",
    "    def gradient_descent(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.theta = np.zeros((n, 1))\n",
    "\n",
    "        for _ in range(self.num_iterations):\n",
    "            h = self.sigmoid(np.dot(X, self.theta))\n",
    "            gradient = np.dot(X.T, (h - y)) / m\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        X = np.concatenate((intercept, X), axis=1)\n",
    "\n",
    "        self.gradient_descent(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        X = np.concatenate((intercept, X), axis=1)\n",
    "\n",
    "        predicted_probs = self.sigmoid(np.dot(X, self.theta))\n",
    "        predicted_labels = (predicted_probs >= self.threshold).astype(int)\n",
    "\n",
    "        return predicted_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T18:14:06.305027700Z",
     "start_time": "2025-04-09T18:14:06.289573Z"
    }
   },
   "id": "75c73f574f7f65b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SGD Regression    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea8d36c3d523b163"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class MySGDRegression:\n",
    "    def __init__(self):\n",
    "        self.intercept_ = 0.0\n",
    "        self.coef_ = []\n",
    "\n",
    "    # simple stochastic GD\n",
    "    def fit(self, x, y, learningRate = 0.001, noEpochs = 1000):\n",
    "        self.coef_ = [0.0 for _ in range(len(x[0]) + 1)]\n",
    "        for epoch in range(noEpochs):\n",
    "            for i in range(len(x)): # for each sample from the training data\n",
    "                ycomputed = self.eval(x[i])     # estimate the output\n",
    "                crtError = ycomputed - y[i]     # compute the error for the current sample\n",
    "                for j in range(0, len(x[0])):   # update the coefficients\n",
    "                    self.coef_[j] = self.coef_[j] - learningRate * crtError * x[i][j]\n",
    "                self.coef_[len(x[0])] = self.coef_[len(x[0])] - learningRate * crtError * 1\n",
    "\n",
    "        self.intercept_ = self.coef_[-1]\n",
    "        self.coef_ = self.coef_[:-1]\n",
    "\n",
    "    def eval(self, xi):\n",
    "        yi = self.coef_[-1]\n",
    "        for j in range(len(xi)):\n",
    "            yi += self.coef_[j] * xi[j]\n",
    "        return yi\n",
    "\n",
    "    def predict(self, x):\n",
    "        yComputed = [self.eval(xi) for xi in x]\n",
    "        return yComputed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T18:14:06.333304100Z",
     "start_time": "2025-04-09T18:14:06.305027700Z"
    }
   },
   "id": "9d00a8fc45e83417"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. univariate gradient descent"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "140ba8b3f9437021"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def loadData(fileName, inputVariabName, outputVariabName):\n",
    "    data = []\n",
    "    dataNames = []\n",
    "    with open(fileName) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                dataNames = row\n",
    "            else:\n",
    "                data.append(row)\n",
    "            line_count += 1\n",
    "    selectedVariable = dataNames.index(inputVariabName)\n",
    "    inputs = [float(data[i][selectedVariable]) for i in range(len(data))]\n",
    "    selectedOutput = dataNames.index(outputVariabName)\n",
    "    outputs = [float(data[i][selectedOutput]) for i in range(len(data))]\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "\n",
    "def plotDataHistogram(x, variableName):\n",
    "    n, bins, patches = plt.hist(x, 10)\n",
    "    plt.title('Histogram of ' + variableName)\n",
    "    plt.show()\n",
    "\n",
    "def plotData(x1, y1, x2 = None, y2 = None, x3 = None, y3 = None, title = None):\n",
    "    plt.plot(x1, y1, 'ro', label = 'train data')\n",
    "    if (x2):\n",
    "        plt.plot(x2, y2, 'b-', label = 'learnt model')\n",
    "    if (x3):\n",
    "        plt.plot(x3, y3, 'g^', label = 'test data')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def univariate_gradient_descent(model):\n",
    "    crtDir = os.getcwd()\n",
    "    filePath = os.path.join(crtDir, 'data', 'world-happiness-report-2017.csv')\n",
    "\n",
    "    inputs, outputs = loadData(filePath, 'Economy..GDP.per.Capita.', 'Happiness.Score')\n",
    "\n",
    "    plotDataHistogram(inputs, 'capita GDP')\n",
    "    plotDataHistogram(outputs, 'Happiness score')\n",
    "\n",
    "    # check the liniarity (to check that a linear relationship exists between the dependent variable (y = happiness) and the independent variable (x = capita).)\n",
    "    plotData(inputs, outputs, [], [], [], [], 'capita vs. hapiness')\n",
    "\n",
    "    # split data into training data (80%) and testing data (20%)\n",
    "    np.random.seed(5)\n",
    "    indexes = [i for i in range(len(inputs))]\n",
    "    trainSample = np.random.choice(indexes, int(0.8 * len(inputs)), replace=False)\n",
    "    validationSample = [i for i in indexes if not i in trainSample]\n",
    "    trainInputs = [inputs[i] for i in trainSample]\n",
    "    trainOutputs = [outputs[i] for i in trainSample]\n",
    "    validationInputs = [inputs[i] for i in validationSample]\n",
    "    validationOutputs = [outputs[i] for i in validationSample]\n",
    "\n",
    "    plotData(trainInputs, trainOutputs, [], [], validationInputs, validationOutputs, \"train and test data\")\n",
    "\n",
    "    # training step\n",
    "    xx = [[el] for el in trainInputs]\n",
    "    if (model == \"stocastic\"):\n",
    "        regressor = MySGDRegression()\n",
    "        # regressor = linear_model.SGDRegressor(max_iter =  10000)\n",
    "    else:\n",
    "        regressor = MyBGDRegression()\n",
    "    regressor.fit(xx, trainOutputs)\n",
    "    w0, w1 = regressor.intercept_, regressor.coef_[0]\n",
    "    print('the learnt model: f(x) = ', w0, ' + ', w1, ' * x')\n",
    "\n",
    "    # plot the model\n",
    "    noOfPoints = 1000\n",
    "    xref = []\n",
    "    val = min(trainInputs)\n",
    "    step = (max(trainInputs) - min(trainInputs)) / noOfPoints\n",
    "    for i in range(1, noOfPoints):\n",
    "        xref.append(val)\n",
    "        val += step\n",
    "    yref = [w0 + w1 * el for el in xref]\n",
    "    plotData(trainInputs, trainOutputs, xref, yref, [], [], title=\"train data and model\")\n",
    "\n",
    "    # makes predictions for test data\n",
    "    # computedTestOutputs = [w0 + w1 * el for el in testInputs]\n",
    "    # makes predictions for test data (by tool)\n",
    "    computedValidationOutputs = regressor.predict([[x] for x in validationInputs])\n",
    "    plotData([], [], validationInputs, computedValidationOutputs, validationInputs, validationOutputs,\n",
    "             \"predictions vs real test data\")\n",
    "\n",
    "    # compute the differences between the predictions and real outputs\n",
    "    error = 0.0\n",
    "    for t1, t2 in zip(computedValidationOutputs, validationOutputs):\n",
    "        error += (t1 - t2) ** 2\n",
    "    error = error / len(validationOutputs)\n",
    "    print(\"prediction error (manual): \", error)\n",
    "\n",
    "    error = mean_squared_error(validationOutputs, computedValidationOutputs)\n",
    "    print(\"prediction error (tool): \", error)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T18:14:14.636702200Z",
     "start_time": "2025-04-09T18:14:06.333304100Z"
    }
   },
   "id": "2c7026af6d3e57ce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. bivariate gradient descent"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "816b748bcbc11d3c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "def plot3Ddata(x1Train, x2Train, yTrain, x1Model = None, x2Model = None, yModel = None, x1Test = None, x2Test = None, yTest = None, title = None):\n",
    "    def remove_negative_values(feature1, feature2, outputs):\n",
    "        new_feature1 = []\n",
    "        new_feature2 = []\n",
    "        new_outputs = []\n",
    "        for f1, f2, out in zip(feature1, feature2, outputs):\n",
    "            if f1 >= 0 and f2 >= 0 and out >= 0:\n",
    "                new_feature1.append(f1)\n",
    "                new_feature2.append(f2)\n",
    "                new_outputs.append(out)\n",
    "        return new_feature1, new_feature2, new_outputs\n",
    "\n",
    "    x1Train, x2Train, yTrain = remove_negative_values(x1Train, x2Train, yTrain)\n",
    "    if x1Test is not None and x2Test is not None and yTest is not None:\n",
    "        x1Test, x2Test, yTest = remove_negative_values(x1Test, x2Test, yTest)\n",
    "\n",
    "    ax = plt.axes(projection = '3d')\n",
    "    if (x1Train):\n",
    "        plt.scatter(x1Train, x2Train, yTrain, c = 'r', marker = 'o', label = 'train data')\n",
    "    if (x1Model):\n",
    "        plt.scatter(x1Model, x2Model, yModel, c = 'b', marker = '_', label = 'learnt model')\n",
    "    if (x1Test):\n",
    "        plt.scatter(x1Test, x2Test, yTest, c = 'g', marker = '^', label = 'test data')\n",
    "    plt.title(title)\n",
    "    ax.set_xlabel(\"capita\")\n",
    "    ax.set_ylabel(\"freedom\")\n",
    "    ax.set_zlabel(\"happiness\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plotDataHistogram(x, variableName):\n",
    "    n, bins, patches = plt.hist(x, 10)\n",
    "    plt.title('Histogram of ' + variableName)\n",
    "    plt.show()\n",
    "\n",
    "def loadDataMoreInputs1(fileName, inputVariabNames, outputVariabName):\n",
    "    data = []\n",
    "    dataNames = []\n",
    "    with open(fileName) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                dataNames = row\n",
    "            else:\n",
    "                data.append(row)\n",
    "            line_count += 1\n",
    "    selectedVariable1 = dataNames.index(inputVariabNames[0])\n",
    "    selectedVariable2 = dataNames.index(inputVariabNames[1])\n",
    "    inputs = [[float(data[i][selectedVariable1]), float(data[i][selectedVariable2])] for i in range(len(data))]\n",
    "    selectedOutput = dataNames.index(outputVariabName)\n",
    "    outputs = [float(data[i][selectedOutput]) for i in range(len(data))]\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "\n",
    "def normalisation(trainData, testData):\n",
    "    scaler = StandardScaler()\n",
    "    if not isinstance(trainData[0], list):\n",
    "        # encode each sample into a list\n",
    "        trainData = [[d] for d in trainData]\n",
    "        testData = [[d] for d in testData]\n",
    "\n",
    "        scaler.fit(trainData)  # fit only on training data\n",
    "        normalisedTrainData = scaler.transform(trainData)  # apply same transformation to train data\n",
    "        normalisedTestData = scaler.transform(testData)  # apply same transformation to test data\n",
    "\n",
    "        # decode from list to raw values\n",
    "        normalisedTrainData = [el[0] for el in normalisedTrainData]\n",
    "        normalisedTestData = [el[0] for el in normalisedTestData]\n",
    "    else:\n",
    "        scaler.fit(trainData)  # fit only on training data\n",
    "        normalisedTrainData = scaler.transform(trainData)  # apply same transformation to train data\n",
    "        normalisedTestData = scaler.transform(testData)  # apply same transformation to test data\n",
    "    return normalisedTrainData, normalisedTestData\n",
    "\n",
    "def bivariate_gradient_descent(model):\n",
    "    # problem hapiness = w0 + w1 * GDPcapita + w2 * freedom\n",
    "    # load data\n",
    "    crtDir = os.getcwd()\n",
    "    filePath = os.path.join(crtDir, 'data', 'world-happiness-report-2017.csv')\n",
    "\n",
    "    inputs, outputs = loadDataMoreInputs1(filePath, ['Economy..GDP.per.Capita.', 'Freedom'], 'Happiness.Score')\n",
    "\n",
    "    feature1 = [ex[0] for ex in inputs]\n",
    "    feature2 = [ex[1] for ex in inputs]\n",
    "\n",
    "    # plot the data histograms\n",
    "    plotDataHistogram(feature1, 'capita GDP')\n",
    "    plotDataHistogram(feature2, 'freedom')\n",
    "    plotDataHistogram(outputs, 'Happiness score')\n",
    "\n",
    "    # check the liniarity (to check that a linear relationship exists between the dependent variable (y = happiness) and the independent variables (x1 = capita, x2 = freedom).)\n",
    "    plot3Ddata(feature1, feature2, outputs, [], [], [], [], [], [], 'capita vs freedom vs happiness')\n",
    "\n",
    "    # PASUL 2: split data into training data (80%) and testing data (20%) and normalise the data\n",
    "    np.random.seed(5)\n",
    "    indexes = [i for i in range(len(inputs))]\n",
    "    trainSample = np.random.choice(indexes, int(0.8 * len(inputs)), replace=False)\n",
    "    testSample = [i for i in indexes if not i in trainSample]\n",
    "\n",
    "    trainInputs = [inputs[i] for i in trainSample]\n",
    "    trainOutputs = [outputs[i] for i in trainSample]\n",
    "    testInputs = [inputs[i] for i in testSample]\n",
    "    testOutputs = [outputs[i] for i in testSample]\n",
    "\n",
    "    trainInputs, testInputs = normalisation(trainInputs, testInputs)\n",
    "    trainOutputs, testOutputs = normalisation(trainOutputs, testOutputs)\n",
    "\n",
    "    feature1train = [ex[0] for ex in trainInputs]\n",
    "    feature2train = [ex[1] for ex in trainInputs]\n",
    "\n",
    "    feature1test = [ex[0] for ex in testInputs]\n",
    "    feature2test = [ex[1] for ex in testInputs]\n",
    "\n",
    "    plot3Ddata(feature1train, feature2train, trainOutputs, [], [], [], feature1test, feature2test, testOutputs,\n",
    "               \"train and test data (after normalisation)\")\n",
    "\n",
    "    # PASUL 3: training step\n",
    "    # identify (by training) the regressor\n",
    "\n",
    "    # # use sklearn regressor\n",
    "    # from sklearn import linear_model\n",
    "    # regressor = linear_model.SGDRegressor()\n",
    "\n",
    "    # using developed code \n",
    "    # model initialisation\n",
    "    if (model == \"stocastic\"):\n",
    "        regressor = MySGDRegression()\n",
    "    else:\n",
    "        regressor = MyBGDRegression()\n",
    "\n",
    "    regressor.fit(trainInputs, trainOutputs)\n",
    "    # print(regressor.coef_)\n",
    "    # print(regressor.intercept_)\n",
    "\n",
    "    # parameters of the liniar regressor\n",
    "    w0, w1, w2 = regressor.intercept_, regressor.coef_[0], regressor.coef_[1]\n",
    "    print('the learnt model: f(x) = ', w0, ' + ', w1, ' * x1 + ', w2, ' * x2')\n",
    "\n",
    "    # PASUL 4: plot the model\n",
    "    # numerical representation of the regressor model\n",
    "    noOfPoints = 50\n",
    "    xref1 = []\n",
    "    val = min(feature1)\n",
    "    step1 = (max(feature1) - min(feature1)) / noOfPoints\n",
    "    for _ in range(1, noOfPoints):\n",
    "        for _ in range(1, noOfPoints):\n",
    "            xref1.append(val)\n",
    "        val += step1\n",
    "\n",
    "    xref2 = []\n",
    "    val = min(feature2)\n",
    "    step2 = (max(feature2) - min(feature2)) / noOfPoints\n",
    "    for _ in range(1, noOfPoints):\n",
    "        aux = val\n",
    "        for _ in range(1, noOfPoints):\n",
    "            xref2.append(aux)\n",
    "            aux += step2\n",
    "    yref = [w0 + w1 * el1 + w2 * el2 for el1, el2 in zip(xref1, xref2)]\n",
    "    plot3Ddata(feature1train, feature2train, trainOutputs, xref1, xref2, yref, [], [], [],\n",
    "               'train data and the learnt model')\n",
    "\n",
    "    # use the trained model to predict new inputs\n",
    "\n",
    "    # makes predictions for test data\n",
    "    # computedTestOutputs = [w0 + w1 * el[0] + w2 * el[1] for el in testInputs]\n",
    "    # makes predictions for test data (by tool)\n",
    "    computedTestOutputs = regressor.predict(testInputs)\n",
    "\n",
    "    plot3Ddata([], [], [], feature1test, feature2test, computedTestOutputs, feature1test, feature2test, testOutputs,\n",
    "               'predictions vs real test data')\n",
    "\n",
    "    # PASUL 5: compute the error\n",
    "    # compute the differences between the predictions and real outputs\n",
    "    error = 0.0\n",
    "    for t1, t2 in zip(computedTestOutputs, testOutputs):\n",
    "        error += (t1 - t2) ** 2\n",
    "    error = error / len(testOutputs)\n",
    "    print('prediction error (manual): ', error)\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    error = mean_squared_error(testOutputs, computedTestOutputs)\n",
    "    print('prediction error (tool):   ', error)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T18:14:14.662015300Z",
     "start_time": "2025-04-09T18:14:14.652762100Z"
    }
   },
   "id": "55dde58849beea8f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. cancerous tissues classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "401fa1a814249fb5"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "def plotDataHistogram(x, variableName):\n",
    "    n, bins, patches = plt.hist(x, 10)\n",
    "    plt.title('Histogram of ' + variableName)\n",
    "    plt.show()\n",
    "\n",
    "def loadDataMoreInputs2(fileName, inputVariabNames, outputVariabName):\n",
    "    data = []\n",
    "    dataNames = []\n",
    "    with open(fileName) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                dataNames = row\n",
    "            else:\n",
    "                data.append(row)\n",
    "            line_count += 1\n",
    "    selectedVariable1 = dataNames.index(inputVariabNames[0])\n",
    "    selectedVariable2 = dataNames.index(inputVariabNames[1])\n",
    "    inputs = [[float(data[i][selectedVariable1]), float(data[i][selectedVariable2])] for i in range(len(data))]\n",
    "    selectedOutput = dataNames.index(outputVariabName)\n",
    "    outputs = [1 if data[i][selectedOutput] == 'M' else 0 for i in range(len(data))]\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "def plotROCCurve(fpr, tpr, roc_auc):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def cancerous_tissues_classification(modelType):\n",
    "    crtDir = os.getcwd()\n",
    "    filePath = os.path.join(crtDir, 'data', 'wdbc.csv')\n",
    "\n",
    "    inputs, outputs = loadDataMoreInputs2(filePath, ['Radius', 'Texture'], 'Diagnosis')\n",
    "\n",
    "    # PASUL 2: split data into training data (80%) and testing data (20%) and normalise data\n",
    "    np.random.seed(5)\n",
    "    indexes = [i for i in range(len(inputs))]\n",
    "    trainSample = np.random.choice(indexes, int(0.8 * len(inputs)), replace=False)\n",
    "    testSample = [i for i in indexes if not i in trainSample]\n",
    "\n",
    "    trainInputs = [inputs[i] for i in trainSample]\n",
    "    trainOutputs = [outputs[i] for i in trainSample]\n",
    "    testInputs = [inputs[i] for i in testSample]\n",
    "    testOutputs = [outputs[i] for i in testSample]\n",
    "\n",
    "    # Normalization\n",
    "    scaler = StandardScaler()\n",
    "    if not isinstance(trainInputs[0], list):\n",
    "        trainInputs = [[d] for d in trainInputs]\n",
    "        testInputs = [[d] for d in testInputs]\n",
    "\n",
    "        scaler.fit(trainInputs)\n",
    "        trainInputs = scaler.transform(trainInputs)\n",
    "        testInputs = scaler.transform(testInputs)\n",
    "\n",
    "        trainInputs = [el[0] for el in trainInputs]\n",
    "        testInputs = [el[0] for el in testInputs]\n",
    "    else:\n",
    "        scaler.fit(trainInputs)\n",
    "        trainInputs = scaler.transform(trainInputs)\n",
    "        testInputs = scaler.transform(testInputs)\n",
    "\n",
    "    # PASUL 3: training step\n",
    "    if modelType == \"tool\":\n",
    "        model = LogisticRegression()\n",
    "        model.fit(trainInputs, trainOutputs.ravel())\n",
    "        w0, w1 = model.intercept_, model.coef_[0]\n",
    "        print('the learnt model: f(x) = ', w0[0], ' + ', w1[0], ' * x1 + ', w1[1], ' * x2')\n",
    "    else:\n",
    "        model = MyLogisticRegression1(thresholds=[0.2, 0.5, 0.9])\n",
    "        trainOutputs = np.array(trainOutputs)\n",
    "        model.fit(trainInputs, trainOutputs)\n",
    "        w0 = model.theta[0]\n",
    "        w1 = model.theta[1]\n",
    "        w2 = model.theta[2]\n",
    "        print('the learnt model: f(x) = ', w0, ' + ', w1, ' * x1 + ', w2, ' * x2')\n",
    "\n",
    "    computedTestOutputs = model.predict(testInputs)\n",
    "\n",
    "    print('Accuracy: ', accuracy_score(testOutputs, computedTestOutputs))  # correct predictions / total predictions\n",
    "    print('Precision: ', precision_score(testOutputs, computedTestOutputs, zero_division=0))\n",
    "  # positive predictions that were correct\n",
    "    print('Recall: ', recall_score(testOutputs, computedTestOutputs))  # correct positive predictions\n",
    "\n",
    "    # Verification for a new input\n",
    "    normalized_inputs = scaler.transform([[18, 10]])\n",
    "    prediction = model.predict(np.array(normalized_inputs))\n",
    "    if prediction[0] == 0:\n",
    "        print(\"The lesion is predicted to be benign.\")\n",
    "    else:\n",
    "        print(\"The lesion is predicted to be malignant.\")\n",
    "\n",
    "    # fpr, tpr, thresholds = roc_curve(testOutputs, computedTestOutputs)\n",
    "    # roc_auc = auc(fpr, tpr)\n",
    "    # plotROCCurve(fpr, tpr, roc_auc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T18:14:15.036139200Z",
     "start_time": "2025-04-09T18:14:14.670031600Z"
    }
   },
   "id": "2e50a122bbe917f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. flower preference clasification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ae43cd430b7b37a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "\n",
    "def loadDataMoreInputs3(fileName, inputVariabNames, outputVariabName, label_encoder):\n",
    "    data = []\n",
    "    dataNames = []\n",
    "    with open(fileName) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                dataNames = row\n",
    "            else:\n",
    "                data.append(row)\n",
    "            line_count += 1\n",
    "    selectedVariables = [dataNames.index(var) for var in inputVariabNames]\n",
    "    inputs = [[float(data[i][var]) for var in selectedVariables] for i in range(len(data))]\n",
    "    selectedOutput = dataNames.index(outputVariabName)\n",
    "    outputs = [data[i][selectedOutput] for i in range(len(data))]\n",
    "\n",
    "    # label_encoder = LabelEncoder()\n",
    "    outputs_encoded = label_encoder.fit_transform(outputs)\n",
    "\n",
    "    outputs_encoded = outputs_encoded.reshape(-1, 1)\n",
    "\n",
    "    return inputs, outputs_encoded\n",
    "\n",
    "def flower_preference_classification(modelType):\n",
    "    crtDir = os.getcwd()\n",
    "    filePath = os.path.join(crtDir, 'data', 'iris.csv')\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    inputs, outputs = loadDataMoreInputs3(filePath, ['SepalLength', 'SepalWidth','PetalLength', 'PetalWidth'], 'Class', label_encoder)\n",
    "\n",
    "    # PASUL 2: split data into training data (80%) and testing data (20%) and normalise data\n",
    "    np.random.seed(5)\n",
    "    indexes = [i for i in range(len(inputs))]\n",
    "    trainSample = np.random.choice(indexes, int(0.8 * len(inputs)), replace=False)\n",
    "    testSample = [i for i in indexes if not i in trainSample]\n",
    "\n",
    "    trainInputs = [inputs[i] for i in trainSample]\n",
    "    trainOutputs = [outputs[i] for i in trainSample]\n",
    "    testInputs = [inputs[i] for i in testSample]\n",
    "    testOutputs = [outputs[i] for i in testSample]\n",
    "\n",
    "    # Normalization\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(trainInputs)\n",
    "    trainInputs = scaler.transform(trainInputs)\n",
    "    testInputs = scaler.transform(testInputs)\n",
    "\n",
    "    # PASUL 3: training step\n",
    "    if modelType == \"tool\":\n",
    "        model = LogisticRegression()\n",
    "        model.fit(trainInputs, trainOutputs)\n",
    "        w0, w1 = model.intercept_, model.coef_[0]\n",
    "        print('the learnt model: f(x) = ', w0[0], ' + ', w1[0], ' * x1 + ', w1[1], ' * x2')\n",
    "    else:\n",
    "        # Custom Logistic Regression model implementation can be used here\n",
    "        model = MyLogisticRegression2(learning_rate=0.01, num_iterations=1000, threshold=0.33)\n",
    "        model.fit(trainInputs, trainOutputs)\n",
    "        learned_coefficients = model.theta\n",
    "        print('the learnt model: f(x) = ', learned_coefficients[0][0], ' + ', learned_coefficients[1][0], ' * x1 + ',\n",
    "              learned_coefficients[2][0], ' * x2')\n",
    "\n",
    "\n",
    "    computedTestOutputs = model.predict(testInputs)\n",
    "\n",
    "    print('Accuracy: ', accuracy_score(testOutputs, computedTestOutputs))  # correct predictions / total predictions\n",
    "    print('Precision: ', precision_score(testOutputs, computedTestOutputs, average='weighted'))  # positive predictions that were correct\n",
    "    print('Recall: ', recall_score(testOutputs, computedTestOutputs, average='weighted'))  # correct positive predictions\n",
    "\n",
    "    # Verification for a new input\n",
    "    normalized_inputs = scaler.transform([[5.35, 3.85, 1.25, 0.4]])\n",
    "    prediction = model.predict(np.array(normalized_inputs))\n",
    "    predicted_species = label_encoder.inverse_transform(prediction)\n",
    "    print(\"The predicted species for the flower is: \", predicted_species[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T18:14:15.049076100Z",
     "start_time": "2025-04-09T18:14:15.036139200Z"
    }
   },
   "id": "61b376197cefd42d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optional : cancerous tissues classification cross validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51352ed2c156291e"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "def loadDataMoreInputs4(fileName, inputVariabNames, outputVariabName):\n",
    "    data = []\n",
    "    dataNames = []\n",
    "    with open(fileName) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                dataNames = row\n",
    "            else:\n",
    "                data.append(row)\n",
    "            line_count += 1\n",
    "    selectedVariable1 = dataNames.index(inputVariabNames[0])\n",
    "    selectedVariable2 = dataNames.index(inputVariabNames[1])\n",
    "    inputs = [[float(data[i][selectedVariable1]), float(data[i][selectedVariable2])] for i in range(len(data))]\n",
    "    selectedOutput = dataNames.index(outputVariabName)\n",
    "    outputs = [1 if data[i][selectedOutput] == 'M' else 0 for i in range(len(data))]\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "def cancerous_tissues_classification_cross_validation():\n",
    "    crtDir = os.getcwd()\n",
    "    filePath = os.path.join(crtDir, 'data', 'wdbc.csv')\n",
    "\n",
    "    inputs, outputs = loadDataMoreInputs4(filePath, ['Radius', 'Texture'], 'Diagnosis')\n",
    "\n",
    "    # Normalization\n",
    "    scaler = StandardScaler()\n",
    "    inputs = scaler.fit_transform(inputs)\n",
    "\n",
    "    # PASUL 3: training step\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "    scores = cross_val_score(model, inputs, outputs, cv=kf, scoring='accuracy')\n",
    "\n",
    "    print(\"Accuracy for each fold: \", scores)\n",
    "    print(\"Mean accuracy: \", scores.mean())\n",
    "\n",
    "    model.fit(inputs, outputs)\n",
    "    print(\"the learnt model: f(x) = \", model.intercept_[0], \" + \", model.coef_[0][0], \" * x1 + \", model.coef_[0][1], \" * x2\")\n",
    "\n",
    "    # Verification for a new input\n",
    "    normalized_inputs = scaler.transform([[18, 10]])\n",
    "    prediction = model.predict(np.array(normalized_inputs))\n",
    "    if prediction[0] == 0:\n",
    "        print(\"The lesion is predicted to be benign.\")\n",
    "    else:\n",
    "        print(\"The lesion is predicted to be malignant.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T18:14:15.062238200Z",
     "start_time": "2025-04-09T18:14:15.052236300Z"
    }
   },
   "id": "566885ee26bc396a"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pb1a: Univariate Gradient Descent\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Personal PC\\\\Videos\\\\an2 sem1\\\\probabilitati si statistica\\\\pythonProject\\\\data\\\\world-happiness-report-2017.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mPb1a: Univariate Gradient Descent\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43munivariate_gradient_descent\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mbatches\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# STOCASTIC:\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# the learnt model: f(x) =  3.1994285956915123  +  2.1487678365481915  * x\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# prediction error (manual):  1.9008773201208433\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     10\u001B[39m \u001B[38;5;66;03m# prediction error (manual):  1.8999752100241594\u001B[39;00m\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m# prediction error (tool):  1.8999752100241594\u001B[39;00m\n\u001B[32m     13\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mPb1b: Bivariate Gradient Descent\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 46\u001B[39m, in \u001B[36munivariate_gradient_descent\u001B[39m\u001B[34m(model)\u001B[39m\n\u001B[32m     43\u001B[39m crtDir = os.getcwd()\n\u001B[32m     44\u001B[39m filePath = os.path.join(crtDir, \u001B[33m'\u001B[39m\u001B[33mdata\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mworld-happiness-report-2017.csv\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m46\u001B[39m inputs, outputs = \u001B[43mloadData\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilePath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mEconomy..GDP.per.Capita.\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mHappiness.Score\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     48\u001B[39m plotDataHistogram(inputs, \u001B[33m'\u001B[39m\u001B[33mcapita GDP\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     49\u001B[39m plotDataHistogram(outputs, \u001B[33m'\u001B[39m\u001B[33mHappiness score\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 10\u001B[39m, in \u001B[36mloadData\u001B[39m\u001B[34m(fileName, inputVariabName, outputVariabName)\u001B[39m\n\u001B[32m      8\u001B[39m data = []\n\u001B[32m      9\u001B[39m dataNames = []\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfileName\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m csv_file:\n\u001B[32m     11\u001B[39m     csv_reader = csv.reader(csv_file, delimiter=\u001B[33m'\u001B[39m\u001B[33m,\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     12\u001B[39m     line_count = \u001B[32m0\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Videos\\an2 sem1\\probabilitati si statistica\\pythonProject\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:325\u001B[39m, in \u001B[36m_modified_open\u001B[39m\u001B[34m(file, *args, **kwargs)\u001B[39m\n\u001B[32m    318\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m}:\n\u001B[32m    319\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    320\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mIPython won\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m by default \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    321\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    322\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33myou can use builtins\u001B[39m\u001B[33m'\u001B[39m\u001B[33m open.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    323\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m325\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Personal PC\\\\Videos\\\\an2 sem1\\\\probabilitati si statistica\\\\pythonProject\\\\data\\\\world-happiness-report-2017.csv'"
     ]
    }
   ],
   "source": [
    "    print(\"\\nPb1a: Univariate Gradient Descent\")\n",
    "    univariate_gradient_descent(\"batches\")\n",
    "    # STOCASTIC:\n",
    "    # the learnt model: f(x) =  3.1994285956915123  +  2.1487678365481915  * x\n",
    "    # prediction error (manual):  1.9008773201208433\n",
    "    # prediction error (tool):  1.9008773201208433\n",
    "\n",
    "    # BATCHES:\n",
    "    # the learnt model: f(x) =  3.199548185216422  +  2.1489553125335217  * x\n",
    "    # prediction error (manual):  1.8999752100241594\n",
    "    # prediction error (tool):  1.8999752100241594\n",
    "\n",
    "    print(\"\\nPb1b: Bivariate Gradient Descent\")\n",
    "    bivariate_gradient_descent(\"batches\")\n",
    "    # STOCASTIC:\n",
    "    # the learnt model: f(x) =  -0.0014527924544318889  +  0.6978631617347402  * x1 +  0.30375393537641193  * x2\n",
    "    # prediction error (manual):  0.2331793993161317\n",
    "    # prediction error (tool):    0.23317939931613166\n",
    "\n",
    "    # BATCHES:\n",
    "    # the learnt model: f(x) =  -0.0011659051562902811  +  0.6979590980445686  * x1 +  0.3039745892925814  * x2\n",
    "    # prediction error (manual):  0.23322521427524043\n",
    "    # prediction error (tool):    0.2332252142752404\n",
    "\n",
    "    print(\"\\nPb2: Clasificarea tesuturilor cancerigene\")\n",
    "    cancerous_tissues_classification(\"manual\")\n",
    "    # CU TOOL:\n",
    "    # the learnt model: f(x) =  -0.9122440356107672  +  3.714265538441941  * x1 +  0.9215248354552286  * x2\n",
    "    # Accuracy: 0.7982456140350878\n",
    "    # The lesion is predicted to be malignant.\n",
    "\n",
    "    # MANUAL:\n",
    "    # the learnt model: f(x) =  -0.960872493582651  +  4.476084448566163  * x1 +  1.04464469245949  * x2\n",
    "    # Accuracy: 0.8070175438596491\n",
    "    # The lesion is predicted to be malignant.\n",
    "\n",
    "    print(\"\\nPb3: Ce fel de floare preferi?\")\n",
    "    flower_preference_classification(\"manual\")\n",
    "    # CU TOOL:\n",
    "    # the learnt model: f(x) =  -0.10971986973912605  +  -0.9687222535134714  * x1 +  1.216084167441676  * x2\n",
    "    # Accuracy:  0.9666666666666667\n",
    "    # Precision:  0.9666666666666667\n",
    "    # Recall:  0.9666666666666667\n",
    "    # The predicted species for the flower is:  Iris-setosa\n",
    "\n",
    "    # MANUAL:\n",
    "    # the learnt model: f(x) =  3.491314967123042  +  3.1000205007035575  * x1 +  -1.0350808485476393  * x2\n",
    "    # Accuracy:  0.5666666666666667\n",
    "    # Precision:  0.415\n",
    "    # Recall:  0.5666666666666667\n",
    "    # The predicted species for the flower is:  Iris-setosa\n",
    "\n",
    "    print(\"\\nOptional: Clasificarea tesuturilor cancerigene cu cross-validation\")\n",
    "    cancerous_tissues_classification_cross_validation()\n",
    "    # Accuracy:  0.887408787455364\n",
    "    # the learnt model: f(x) =  -0.6994318055050132  +  3.336421768653383  * x1 +  0.8767014817409343  * x2\n",
    "    # The lesion is predicted to be malignant."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T18:14:15.923986900Z",
     "start_time": "2025-04-09T18:14:15.065493900Z"
    }
   },
   "id": "b1615b60378635dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
