{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Stabilirea sentimentului folosind Azure"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbd71efe810bd985"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "key =\"FIWuXT3fz0I3Yn0k4YVqy7nTALobYwJvPMYJQzjJyy24hmSRWUNoJQQJ99BEAC5RqLJXJ3w3AAAaACOGbPt0\"\n",
    "endpoint =\"https://georgianaa.cognitiveservices.azure.com/\"\n",
    "\n",
    "def authenticate():\n",
    "    credential = AzureKeyCredential(key)\n",
    "    return TextAnalyticsClient(endpoint=endpoint, credential=credential)\n",
    "\n",
    "def azure_sentiment_analysis(text):\n",
    "    client = authenticate()\n",
    "    document = [text]\n",
    "    response = client.analyze_sentiment(documents=document)[0]\n",
    "    print(f\"\\nAzure Sentiment: {response.sentiment}\")\n",
    "    for key, val in response.confidence_scores.__dict__.items():\n",
    "        print(f\" {key.capitalize()}: {val:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-01T13:09:59.538230200Z",
     "start_time": "2025-05-01T13:09:59.505066500Z"
    }
   },
   "id": "a06bcb68ea88fecc"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Azure Sentiment: positive\n",
      " Positive: 0.87\n",
      " Neutral: 0.13\n",
      " Negative: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Test Azure\n",
    "msg = \"By choosing a bike over a car, I’m reducing my environmental footprint. Cycling promotes eco-friendly transportation, and I’m proud to be part of that movement.\"\n",
    "azure_sentiment_analysis(msg)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-01T13:16:13.214801400Z",
     "start_time": "2025-05-01T13:16:12.754222500Z"
    }
   },
   "id": "e2e872ce12975108"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extragerea caracteristicilor BoW / TF-IDF / Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66e2cdeeae22630d"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Personal\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Personal\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Personal\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Preprocesare text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-01T13:10:07.523130800Z",
     "start_time": "2025-05-01T13:10:07.508936200Z"
    }
   },
   "id": "62253dd8087f4b3c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Încărcare dataset și curățare\n",
    "df = pd.read_csv(\"data/reviews_mixed.csv\")\n",
    "df.dropna(inplace=True)\n",
    "df['cleaned'] = df['Text'].apply(clean_text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-01T13:10:12.162538Z",
     "start_time": "2025-05-01T13:10:12.121072Z"
    }
   },
   "id": "70dd5691417cb819"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# BoW\n",
    "vectorizer_bow = CountVectorizer(max_features=1000)\n",
    "X_bow = vectorizer_bow.fit_transform(df['cleaned']).toarray()\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(df['cleaned']).toarray()\n",
    "\n",
    "# Word2Vec\n",
    "sentences = [text.split() for text in df['cleaned']]\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "X_w2v = np.array([\n",
    "    np.mean([w2v_model.wv[word] for word in words if word in w2v_model.wv] or [np.zeros(100)], axis=0)\n",
    "    for words in sentences\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-01T13:10:14.465899700Z",
     "start_time": "2025-05-01T13:10:14.370704500Z"
    }
   },
   "id": "99c9a87d0272cdf4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Alte caracteristici"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1968afbac7c4ac6"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "positive_words = {'happy', 'joy', 'love', 'great', 'good', 'excellent', 'amazing'}\n",
    "negative_words = {'sad', 'bad', 'terrible', 'awful', 'hate', 'worst'}\n",
    "\n",
    "def extra_features(text):\n",
    "    tokens = text.split()\n",
    "    word_count = len(tokens)\n",
    "    char_count = len(text)\n",
    "    pos_count = sum(1 for word in tokens if word in positive_words)\n",
    "    neg_count = sum(1 for word in tokens if word in negative_words)\n",
    "    return [char_count, word_count, pos_count, neg_count]\n",
    "\n",
    "X_extra = np.array([extra_features(text) for text in df['cleaned']])\n",
    "scaler = StandardScaler()\n",
    "X_extra_scaled = scaler.fit_transform(X_extra)\n",
    "\n",
    "# Combinare TF-IDF + extra features\n",
    "X_combined = np.hstack((X_tfidf, X_extra_scaled))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-01T13:10:21.726907700Z",
     "start_time": "2025-05-01T13:10:21.714880700Z"
    }
   },
   "id": "8b8a886dc03ae187"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Clasificator ANN (tool – Keras) + predicție mesaj"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a9c8519a296bfd1"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.src.models import Sequential\n",
    "from keras.src.layers import Dense, Dropout\n",
    "from keras.src.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Etichetare\n",
    "label_encoder = LabelEncoder()\n",
    "y_labels = label_encoder.fit_transform(df[\"Sentiment\"])\n",
    "y = to_categorical(y_labels)\n",
    "\n",
    "# Împărțire date\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-01T13:10:24.207116900Z",
     "start_time": "2025-05-01T13:10:24.192338100Z"
    }
   },
   "id": "1af6f52a51b13f7e"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Personal PC\\Videos\\an2 sem1\\probabilitati si statistica\\pythonProject\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 37ms/step - accuracy: 0.3989 - loss: 0.7197 - val_accuracy: 0.4118 - val_loss: 0.6976\n",
      "Epoch 2/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.6661 - loss: 0.6873 - val_accuracy: 0.6471 - val_loss: 0.6727\n",
      "Epoch 3/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.7425 - loss: 0.6445 - val_accuracy: 0.7059 - val_loss: 0.6499\n",
      "Epoch 4/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.7304 - loss: 0.6233 - val_accuracy: 0.7059 - val_loss: 0.6300\n",
      "Epoch 5/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.6574 - loss: 0.6230 - val_accuracy: 0.7059 - val_loss: 0.6120\n",
      "Epoch 6/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.7059 - loss: 0.5902 - val_accuracy: 0.7059 - val_loss: 0.5938\n",
      "Epoch 7/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.6994 - loss: 0.5623 - val_accuracy: 0.7059 - val_loss: 0.5768\n",
      "Epoch 8/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.6882 - loss: 0.5433 - val_accuracy: 0.7059 - val_loss: 0.5570\n",
      "Epoch 9/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.7549 - loss: 0.4848 - val_accuracy: 0.7059 - val_loss: 0.5356\n",
      "Epoch 10/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.7422 - loss: 0.4600 - val_accuracy: 0.7059 - val_loss: 0.5106\n",
      "Epoch 11/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.8059 - loss: 0.4149 - val_accuracy: 0.8235 - val_loss: 0.4858\n",
      "Epoch 12/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.8634 - loss: 0.3910 - val_accuracy: 0.8235 - val_loss: 0.4640\n",
      "Epoch 13/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.9195 - loss: 0.3334 - val_accuracy: 0.8235 - val_loss: 0.4484\n",
      "Epoch 14/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.9413 - loss: 0.2828 - val_accuracy: 0.8235 - val_loss: 0.4350\n",
      "Epoch 15/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.9522 - loss: 0.2404 - val_accuracy: 0.7647 - val_loss: 0.4193\n",
      "Epoch 16/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.9765 - loss: 0.1915 - val_accuracy: 0.7647 - val_loss: 0.4080\n",
      "Epoch 17/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.9690 - loss: 0.1753 - val_accuracy: 0.7647 - val_loss: 0.4020\n",
      "Epoch 18/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9801 - loss: 0.1250 - val_accuracy: 0.7647 - val_loss: 0.4001\n",
      "Epoch 19/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.9864 - loss: 0.1058 - val_accuracy: 0.7647 - val_loss: 0.4044\n",
      "Epoch 20/20\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.9943 - loss: 0.0954 - val_accuracy: 0.7647 - val_loss: 0.4014\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.history.History at 0x19a3d1b3110>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model ANN cu Keras\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_combined.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(y_train.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=20, validation_split=0.1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-01T13:16:39.486710800Z",
     "start_time": "2025-05-01T13:16:35.537083700Z"
    }
   },
   "id": "be18e2e4a002f883"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step - accuracy: 0.7946 - loss: 0.5993\n",
      "\n",
      "Accuracy (Keras ANN): 0.79\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 74ms/step\n",
      "\n",
      "Predicted Sentiment (Keras): positive\n"
     ]
    }
   ],
   "source": [
    "# Evaluare + predicție\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nAccuracy (Keras ANN): {acc:.2f}\")\n",
    "\n",
    "msg_cleaned = clean_text(msg)\n",
    "msg_vec = vectorizer_tfidf.transform([msg_cleaned]).toarray()\n",
    "msg_extra = scaler.transform([extra_features(msg_cleaned)])\n",
    "msg_input = np.hstack((msg_vec, msg_extra))\n",
    "\n",
    "pred = model.predict(msg_input)\n",
    "pred_label = label_encoder.inverse_transform([np.argmax(pred)])\n",
    "print(f\"\\nPredicted Sentiment (Keras): {pred_label[0]}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-01T13:16:42.650026300Z",
     "start_time": "2025-05-01T13:16:42.403740100Z"
    }
   },
   "id": "67d46a11d0e921dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clasificator ANN (manual – fără tool) + predicție mesaj"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ff84dfe065708f4"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# ANN manual\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "class ANN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=0.001):\n",
    "        self.lr = lr\n",
    "        self.W1 = np.random.randn(input_size, hidden_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        m = y.shape[0]\n",
    "        d_z2 = output - y\n",
    "        d_W2 = self.a1.T @ d_z2 / m\n",
    "        d_b2 = np.sum(d_z2, axis=0, keepdims=True) / m\n",
    "        d_z1 = (d_z2 @ self.W2.T) * sigmoid_derivative(self.z1)\n",
    "        d_W1 = X.T @ d_z1 / m\n",
    "        d_b1 = np.sum(d_z1, axis=0, keepdims=True) / m\n",
    "\n",
    "        self.W1 -= self.lr * d_W1\n",
    "        self.b1 -= self.lr * d_b1\n",
    "        self.W2 -= self.lr * d_W2\n",
    "        self.b2 -= self.lr * d_b2\n",
    "\n",
    "    def train(self, X, y, epochs=100):\n",
    "        for i in range(epochs):\n",
    "            out = self.forward(X)\n",
    "            self.backward(X, y, out)\n",
    "            if i % 10 == 0:\n",
    "                loss = np.mean((y - out) ** 2)\n",
    "                print(f\"Epoch {i} - Loss: {loss:.4f}\")\n",
    "# ANN manual\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "class ANN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=0.001):\n",
    "        self.lr = lr\n",
    "        self.W1 = np.random.randn(input_size, hidden_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        m = y.shape[0]\n",
    "        d_z2 = output - y\n",
    "        d_W2 = self.a1.T @ d_z2 / m\n",
    "        d_b2 = np.sum(d_z2, axis=0, keepdims=True) / m\n",
    "        d_z1 = (d_z2 @ self.W2.T) * sigmoid_derivative(self.z1)\n",
    "        d_W1 = X.T @ d_z1 / m\n",
    "        d_b1 = np.sum(d_z1, axis=0, keepdims=True) / m\n",
    "\n",
    "        self.W1 -= self.lr * d_W1\n",
    "        self.b1 -= self.lr * d_b1\n",
    "        self.W2 -= self.lr * d_W2\n",
    "        self.b2 -= self.lr * d_b2\n",
    "\n",
    "    def train(self, X, y, epochs=100):\n",
    "        for i in range(epochs):\n",
    "            out = self.forward(X)\n",
    "            self.backward(X, y, out)\n",
    "            if i % 10 == 0:\n",
    "                loss = np.mean((y - out) ** 2)\n",
    "                print(f\"Epoch {i} - Loss: {loss:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-01T13:16:48.928727400Z",
     "start_time": "2025-05-01T13:16:48.923058200Z"
    }
   },
   "id": "8624eab6d91c06db"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss: 0.3670\n",
      "Epoch 10 - Loss: 0.3659\n"
     ]
    }
   ],
   "source": [
    "# Antrenare model manual\n",
    "X_small = X_combined[:209]\n",
    "y_small = y[:209]\n",
    "ann = ANN(input_size=X_small.shape[1], hidden_size=32, output_size=2)\n",
    "ann.train(X_small, y_small, epochs=20)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-01T13:16:51.017569200Z",
     "start_time": "2025-05-01T13:16:50.954717200Z"
    }
   },
   "id": "af65d278d186c1ca"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Sentiment (Manual ANN): positive\n"
     ]
    }
   ],
   "source": [
    "# Predicție cu ANN manual\n",
    "manual_pred = ann.forward(msg_input)\n",
    "manual_label = np.argmax(manual_pred)\n",
    "if y.shape[1] == 2:\n",
    "    label_name = label_encoder.inverse_transform([manual_label])[0]\n",
    "else:\n",
    "    label_name = f\"Clasa {manual_label}\"\n",
    "\n",
    "print(f\"\\nPredicted Sentiment (Manual ANN): {label_name}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-01T13:16:53.537187700Z",
     "start_time": "2025-05-01T13:16:53.505050100Z"
    }
   },
   "id": "7c582a4bf2a1f562"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e4fa87d53994a55a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
